{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CUDA:0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import abc\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import linear_model, model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import multiprocessing as mp\n",
    "from scipy.stats import norm, kurtosis, skew\n",
    "from progress.bar import Bar as Bar\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "import deepcore.nets as nets\n",
    "import deepcore.datasets as datasets\n",
    "import deepcore.methods as methods\n",
    "from utils import *\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from RelaxLoss.source import cifar\n",
    "from RelaxLoss.source.utils.misc import *\n",
    "from RelaxLoss.source.utils.base import BaseTrainer\n",
    "from RelaxLoss.source.utils.logger import AverageMeter, Logger\n",
    "from RelaxLoss.source.utils.eval import accuracy, accuracy_binary, metrics_binary\n",
    "from RelaxLoss.source.cifar import models\n",
    "from RelaxLoss.source import utils\n",
    "from RelaxLoss.source.cifar import defense\n",
    "from RelaxLoss.source.cifar.dataset import CIFAR10, CIFAR100\n",
    "# from RelaxLoss.source.cifar import run_attacks\n",
    "import metric\n",
    "\n",
    "device_ids = [7]\n",
    "# torch.cuda.set_device(device_ids[0])\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())\n",
    "\n",
    "RNG = torch.Generator().manual_seed(42)\n",
    "mp.set_start_method('spawn',force=True)\n",
    "\n",
    "sys.path.append(\"DeepCore/RelaxLoss/source/cifar\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=argparse.Namespace()\n",
    "args.device=DEVICE\n",
    "# args.selection_batch=None\n",
    "args.workers=4\n",
    "args.dataset='CIFAR10'\n",
    "args.data_path='data'\n",
    "args.selection_epochs=1\n",
    "args.uncertainty=\"Entropy\"\n",
    "args.balance=False\n",
    "args.submodular_greedy=\"LazyGreedy\"\n",
    "args.submodular=\"GraphCut\"\n",
    "args.selection=\"Uniform\"\n",
    "args.fraction=1\n",
    "# args.seed=int(time.time() * 1000) % 100000\n",
    "args.seed=1000\n",
    "args.print_freq=20\n",
    "args.batch=256\n",
    "args.selection_batch=256\n",
    "args.model='ResNet18'\n",
    "args.gpu=[0]\n",
    "args.selection_optimizer=\"SGD\"\n",
    "args.selection_lr=0.1\n",
    "args.selection_weight_decay=5e-4\n",
    "args.selection_nesterov=True\n",
    "args.selection_momentum=0.9\n",
    "\n",
    "args.method='advreg'\n",
    "args.test_batchsize=64\n",
    "args.random_seed=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# download and pre-process CIFAR10\n",
    "normalize = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=normalize\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# we split held out data into test and validation set\n",
    "held_out = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=normalize\n",
    ")\n",
    "test_set, val_set = torch.utils.data.random_split(held_out, [0.5, 0.5], generator=RNG)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# for the unlearning algorithm we'll also need a split of the train set into\n",
    "# forget_set and a retain_set\n",
    "forget_set, retain_set = torch.utils.data.random_split(train_set, [0.1, 0.9], generator=RNG)\n",
    "forget_loader = torch.utils.data.DataLoader(\n",
    "    forget_set, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "retain_loader = torch.utils.data.DataLoader(\n",
    "    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download pre-trained weights\n",
    "local_path = \"weights_resnet18_cifar10.pth\"\n",
    "if not os.path.exists(local_path):\n",
    "    response = requests.get(\n",
    "        \"https://unlearning-challenge.s3.eu-west-1.amazonaws.com/weights_resnet18_cifar10.pth\"\n",
    "    )\n",
    "    open(local_path, \"wb\").write(response.content)\n",
    "\n",
    "weights_pretrained = torch.load(local_path, map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(weights=None, num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    unique_members = np.unique(members)\n",
    "    if not np.all(unique_members == np.array([0, 1])):\n",
    "        raise ValueError(\"members should only have 0 and 1s\")\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state)\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "def compute_losses(net, loader):\n",
    "    \"\"\"Auxiliary function to compute per-sample losses\"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    all_losses = []\n",
    "    net=net.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            logits = net(inputs)\n",
    "            losses = criterion(logits, targets).numpy(force=True)\n",
    "            for l in losses:\n",
    "                all_losses.append(l)\n",
    "\n",
    "    return np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark(object):\n",
    "    def __init__(self, shadow_train_scores, shadow_test_scores, target_train_scores, target_test_scores):\n",
    "        self.s_tr_scores = shadow_train_scores\n",
    "        self.s_te_scores = shadow_test_scores\n",
    "        self.t_tr_scores = target_train_scores\n",
    "        self.t_te_scores = target_test_scores\n",
    "        self.num_methods = len(self.s_tr_scores)\n",
    "\n",
    "    def load_labels(self, s_tr_labels, s_te_labels, t_tr_labels, t_te_labels, num_classes):\n",
    "        \"\"\"Load sample labels\"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.s_tr_labels = s_tr_labels\n",
    "        self.s_te_labels = s_te_labels\n",
    "        self.t_tr_labels = t_tr_labels\n",
    "        self.t_te_labels = t_te_labels\n",
    "\n",
    "    def _thre_setting(self, tr_values, te_values):\n",
    "        \"\"\"Select the best threshold\"\"\"\n",
    "        value_list = np.concatenate((tr_values, te_values))\n",
    "        thre, max_acc = 0, 0\n",
    "        for value in value_list:\n",
    "            tr_ratio = np.sum(tr_values >= value) / (len(tr_values) + 0.0)\n",
    "            te_ratio = np.sum(te_values < value) / (len(te_values) + 0.0)\n",
    "            acc = 0.5 * (tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        return thre\n",
    "\n",
    "    def _mem_inf_thre_perclass(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        \"\"\"MIA by thresholding per-class feature values \"\"\"\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        for num in range(self.num_classes):\n",
    "            thre = self._thre_setting(s_tr_values[self.s_tr_labels == num], s_te_values[self.s_te_labels == num])\n",
    "            t_tr_mem += np.sum(t_tr_values[self.t_tr_labels == num] >= thre)\n",
    "            t_te_non_mem += np.sum(t_te_values[self.t_te_labels == num] < thre)\n",
    "        mem_inf_acc = 0.5 * (t_tr_mem / (len(self.t_tr_labels) + 0.0) + t_te_non_mem / (len(self.t_te_labels) + 0.0))\n",
    "        info = 'MIA via {n} (pre-class threshold): the attack acc is {acc:.3f}'.format(n=v_name, acc=mem_inf_acc)\n",
    "        print(info)\n",
    "        return info, mem_inf_acc\n",
    "\n",
    "    def _mem_inf_thre(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        \"\"\"MIA by thresholding overall feature values\"\"\"\n",
    "        t_tr_mem, t_te_non_mem = 0, 0\n",
    "        thre = self._thre_setting(s_tr_values, s_te_values)\n",
    "        t_tr_mem += np.sum(t_tr_values >= thre)\n",
    "        t_te_non_mem += np.sum(t_te_values < thre)\n",
    "        mem_inf_acc = 0.5 * (t_tr_mem / (len(t_tr_values) + 0.0) + t_te_non_mem / (len(t_te_values) + 0.0))\n",
    "        info = 'MIA via {n} (general threshold): the attack acc is {acc:.3f}'.format(n=v_name, acc=mem_inf_acc)\n",
    "        print(info)\n",
    "        return info, mem_inf_acc\n",
    "\n",
    "    def _mem_inf_roc(self, v_name, s_tr_values, s_te_values, t_tr_values, t_te_values):\n",
    "        \"\"\"MIA AUC given the feature values (no need to threshold)\"\"\"\n",
    "        labels = np.concatenate((np.zeros((len(t_te_values),)), np.ones((len(t_tr_values),))))\n",
    "        results = np.concatenate((t_te_values, t_tr_values))\n",
    "        auc = metrics.roc_auc_score(labels, results)\n",
    "        ap = metrics.average_precision_score(labels, results)\n",
    "        info = 'MIA via {n}: the attack auc is {auc:.3f}, ap is {ap:.3f}'.format(n=v_name, auc=auc, ap=ap)\n",
    "        print(info)\n",
    "        return info, auc\n",
    "\n",
    "    def compute_attack_acc(self, method_names, score_signs, if_per_class_thres=False):\n",
    "        \"\"\"Compute Attack accuracy\"\"\"\n",
    "        if if_per_class_thres:\n",
    "            mem_inf_thre_func = self._mem_inf_thre_perclass\n",
    "            loginfo = 'per class threshold\\n'\n",
    "        else:\n",
    "            mem_inf_thre_func = self._mem_inf_thre\n",
    "            loginfo = 'overall threshold\\n'\n",
    "        results = []\n",
    "        for i in range(self.num_methods):\n",
    "            if score_signs[i] == '+':\n",
    "                info, result = mem_inf_thre_func(method_names[i], self.s_tr_scores[i], self.s_te_scores[i],\n",
    "                                                 self.t_tr_scores[i], self.t_te_scores[i])\n",
    "                loginfo += info + '\\n'\n",
    "                results.append(result)\n",
    "\n",
    "            else:\n",
    "                info, result = mem_inf_thre_func(method_names[i], -self.s_tr_scores[i], -self.s_te_scores[i],\n",
    "                                                 -self.t_tr_scores[i], -self.t_te_scores[i])\n",
    "                loginfo += info + '\\n'\n",
    "                results.append(result)\n",
    "        return loginfo, method_names, results\n",
    "\n",
    "    def compute_attack_auc(self, method_names, score_signs):\n",
    "        \"\"\"Compute attack AUC (and AP)\"\"\"\n",
    "        loginfo = ''\n",
    "        results = []\n",
    "        for i in range(self.num_methods):\n",
    "            if score_signs[i] == '+':\n",
    "                info, result = self._mem_inf_roc(method_names[i], self.s_tr_scores[i], self.s_te_scores[i],\n",
    "                                                 self.t_tr_scores[i], self.t_te_scores[i])\n",
    "                loginfo += info + '\\n'\n",
    "                results.append(result)\n",
    "            else:\n",
    "                info, result = self._mem_inf_roc(method_names[i], -self.s_tr_scores[i], -self.s_te_scores[i],\n",
    "                                                 -self.t_tr_scores[i], -self.t_te_scores[i])\n",
    "                loginfo += info + '\\n'\n",
    "                results.append(result)\n",
    "        return loginfo, method_names, results\n",
    "\n",
    "class Benchmark_Blackbox(Benchmark):\n",
    "    def compute_bb_scores(self):\n",
    "        self.s_tr_outputs, self.s_tr_loss = self.s_tr_scores\n",
    "        self.s_te_outputs, self.s_te_loss = self.s_te_scores\n",
    "        self.t_tr_outputs, self.t_tr_loss = self.t_tr_scores\n",
    "        self.t_te_outputs, self.t_te_loss = self.t_te_scores\n",
    "\n",
    "        # whether the prediction is correct [num_samples,]\n",
    "        self.s_tr_corr = (np.argmax(self.s_tr_outputs, axis=1) == self.s_tr_labels).astype(int)\n",
    "        self.s_te_corr = (np.argmax(self.s_te_outputs, axis=1) == self.s_te_labels).astype(int)\n",
    "        self.t_tr_corr = (np.argmax(self.t_tr_outputs, axis=1) == self.t_tr_labels).astype(int)\n",
    "        self.t_te_corr = (np.argmax(self.t_te_outputs, axis=1) == self.t_te_labels).astype(int)\n",
    "\n",
    "        # confidence prediction of the ground-truth class [num_samples,]\n",
    "        self.s_tr_conf = np.array([self.s_tr_outputs[i, self.s_tr_labels[i]] for i in range(len(self.s_tr_labels))])\n",
    "        self.s_te_conf = np.array([self.s_te_outputs[i, self.s_te_labels[i]] for i in range(len(self.s_te_labels))])\n",
    "        self.t_tr_conf = np.array([self.t_tr_outputs[i, self.t_tr_labels[i]] for i in range(len(self.t_tr_labels))])\n",
    "        self.t_te_conf = np.array([self.t_te_outputs[i, self.t_te_labels[i]] for i in range(len(self.t_te_labels))])\n",
    "\n",
    "        # entropy of the prediction [num_samples,]\n",
    "        self.s_tr_entr = self._entr_comp(self.s_tr_outputs)\n",
    "        self.s_te_entr = self._entr_comp(self.s_te_outputs)\n",
    "        self.t_tr_entr = self._entr_comp(self.t_tr_outputs)\n",
    "        self.t_te_entr = self._entr_comp(self.t_te_outputs)\n",
    "\n",
    "        # proposed modified entropy [num_samples,]\n",
    "        self.s_tr_m_entr = self._m_entr_comp(self.s_tr_outputs, self.s_tr_labels)\n",
    "        self.s_te_m_entr = self._m_entr_comp(self.s_te_outputs, self.s_te_labels)\n",
    "        self.t_tr_m_entr = self._m_entr_comp(self.t_tr_outputs, self.t_tr_labels)\n",
    "        self.t_te_m_entr = self._m_entr_comp(self.t_te_outputs, self.t_te_labels)\n",
    "\n",
    "    def _log_value(self, probs, small_value=1e-30):\n",
    "        return -np.log(np.maximum(probs, small_value))\n",
    "\n",
    "    def _entr_comp(self, probs):\n",
    "        \"\"\"compute the entropy of the prediction\"\"\"\n",
    "        return np.sum(np.multiply(probs, self._log_value(probs)), axis=1)\n",
    "\n",
    "    def _m_entr_comp(self, probs, true_labels):\n",
    "        \"\"\"-(1-f(x)_y) log(f(x)_y) - \\sum_i f(x)_i log(1-f(x)_i)\"\"\"\n",
    "\n",
    "        log_probs = self._log_value(probs)\n",
    "        reverse_probs = 1 - probs\n",
    "        log_reverse_probs = self._log_value(reverse_probs)\n",
    "        modified_probs = np.copy(probs)\n",
    "        modified_probs[range(true_labels.size), true_labels] = reverse_probs[range(true_labels.size), true_labels]\n",
    "        modified_log_probs = np.copy(log_reverse_probs)\n",
    "        modified_log_probs[range(true_labels.size), true_labels] = log_probs[range(true_labels.size), true_labels]\n",
    "        return np.sum(np.multiply(modified_probs, modified_log_probs), axis=1)\n",
    "\n",
    "    def _mem_inf_via_corr(self):\n",
    "        \"\"\"perform membership inference attack based on whether the input is correctly classified or not\"\"\"\n",
    "        t_tr_acc = np.sum(self.t_tr_corr) / (len(self.t_tr_corr) + 0.0)\n",
    "        t_te_acc = np.sum(self.t_te_corr) / (len(self.t_te_corr) + 0.0)\n",
    "        mem_inf_acc = 0.5 * (t_tr_acc + 1 - t_te_acc)\n",
    "        info = 'MIA via correctness, the attack acc is {acc1:.3f}, with train acc {acc2:.3f} and test acc {acc3:.3f}'.format(\n",
    "            acc1=mem_inf_acc, acc2=t_tr_acc, acc3=t_te_acc)\n",
    "        print(info)\n",
    "        return info, mem_inf_acc\n",
    "\n",
    "    def compute_attack_acc(self, method_names=[], all_methods=True, if_per_class_thres=True):\n",
    "        \"\"\"Compute Attack accuracy\"\"\"\n",
    "        if if_per_class_thres:\n",
    "            mem_inf_thre_func = self._mem_inf_thre_perclass\n",
    "            loginfo = 'per class threshold\\n'\n",
    "        else:\n",
    "            mem_inf_thre_func = self._mem_inf_thre\n",
    "            loginfo = 'overall threshold\\n'\n",
    "        results = []\n",
    "        methods = []\n",
    "        if (all_methods) or ('correctness' in method_names):\n",
    "            info, result = self._mem_inf_via_corr()\n",
    "            loginfo += info + '\\n'\n",
    "        if (all_methods) or ('confidence' in method_names):\n",
    "            info, result = mem_inf_thre_func('confidence', self.s_tr_conf, self.s_te_conf,\n",
    "                                             self.t_tr_conf, self.t_te_conf)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('confidence ACC')\n",
    "        if (all_methods) or ('entropy' in method_names):\n",
    "            info, result = mem_inf_thre_func('entropy', -self.s_tr_entr, -self.s_te_entr,\n",
    "                                             -self.t_tr_entr, -self.t_te_entr)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('entropy ACC')\n",
    "        if (all_methods) or ('modified entropy' in method_names):\n",
    "            info, result = mem_inf_thre_func('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr,\n",
    "                                             -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('modified entropy ACC')\n",
    "        if (all_methods) or ('loss' in method_names):\n",
    "            info, result = mem_inf_thre_func('loss', -self.s_tr_loss, -self.s_te_loss,\n",
    "                                             -self.t_tr_loss, -self.t_te_loss)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('loss ACC')\n",
    "        return loginfo, methods, results\n",
    "\n",
    "    def compute_attack_auc(self, method_names=[], all_methods=True):\n",
    "        \"\"\"Compute all attack AUC\"\"\"\n",
    "        loginfo = ''\n",
    "        methods = []\n",
    "        results = []\n",
    "        if (all_methods) or ('confidence' in method_names):\n",
    "            info, result = self._mem_inf_roc('confidence', self.s_tr_conf, self.s_te_conf,\n",
    "                                             self.t_tr_conf, self.t_te_conf)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('confidence AUC')\n",
    "        if (all_methods) or ('entropy' in method_names):\n",
    "            info, result = self._mem_inf_roc('entropy', -self.s_tr_entr, -self.s_te_entr,\n",
    "                                             -self.t_tr_entr, -self.t_te_entr)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('entropy AUC')\n",
    "        if (all_methods) or ('modified entropy' in method_names):\n",
    "            info, result = self._mem_inf_roc('modified entropy', -self.s_tr_m_entr, -self.s_te_m_entr,\n",
    "                                             -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('modified entropy AUC')\n",
    "        if (all_methods) or ('loss' in method_names):\n",
    "            info, result = self._mem_inf_roc('loss', -self.s_tr_loss, -self.s_te_loss,\n",
    "                                             -self.t_tr_loss, -self.t_te_loss)\n",
    "            loginfo += info + '\\n'\n",
    "            results.append(result)\n",
    "            methods.append('modified entropy AUC')\n",
    "        return loginfo, methods, results\n",
    "\n",
    "class BaseAttacker(object):\n",
    "    def __init__(self, args, save_dir,member,nonmember):\n",
    "        self.args = args\n",
    "        self.save_dir = save_dir\n",
    "        self.member=member\n",
    "        self.nonmember=nonmember\n",
    "        self.set_cuda_device()\n",
    "        self.set_seed()\n",
    "        self.set_dataloader()\n",
    "        self.set_criterion()\n",
    "        self.load_models()\n",
    "        \n",
    "\n",
    "    def set_cuda_device(self):\n",
    "        \"\"\"The function to set CUDA device.\"\"\"\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        self.device = torch.device(\"cuda:0\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "    def set_criterion(self):\n",
    "        self.crossentropy = nn.CrossEntropyLoss()\n",
    "        self.crossentropy_noreduce = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def set_seed(self):\n",
    "        \"\"\"Set random seed\"\"\"\n",
    "        random.seed(self.args.seed)\n",
    "        torch.manual_seed(self.args.seed)\n",
    "        np.random.seed(self.args.seed)\n",
    "        if self.use_cuda:\n",
    "            torch.cuda.manual_seed_all(self.args.seed)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def set_dataloader(self):\n",
    "        \"\"\"The function to set the dataloader\"\"\"\n",
    "        self.data_root = None\n",
    "        self.dataset = None\n",
    "        self.num_classes = None\n",
    "        self.dataset_size = None\n",
    "        self.transform_train = None\n",
    "        self.transform_test = None\n",
    "        self.target_trainloader = None\n",
    "        self.target_testloader = None\n",
    "        self.shadow_trainloader = None\n",
    "        self.shadow_testloader = None\n",
    "        self.loader_dict = None\n",
    "\n",
    "    def load_models(self):\n",
    "        target_model=models.__dict__[self.args.model](num_classes=self.args.num_classes)\n",
    "        checkpoint = torch.load(os.path.join(self.args.target_path, 'checkpoint.pkl'))\n",
    "        if 'module.' in list(checkpoint['model_state_dict'].keys())[0]:\n",
    "            new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}\n",
    "            target_model.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            target_model.load_state_dict(checkpoint['model_state_dict']) # target_path = os.path.join(self.args.target_path, 'checkpoint.pkl')\n",
    "        self.target_model = target_model\n",
    "        # self.target_model=torch.load(os.path.join(self.args.target_path, 'model.pt'))\n",
    "        print('Loading target model from ', self.args.target_path)\n",
    "        shadow_model=models.__dict__[self.args.model](num_classes=self.args.num_classes)\n",
    "        checkpoint = torch.load(os.path.join(self.args.shadow_path, 'checkpoint.pkl'))\n",
    "        print(list(checkpoint.keys()))\n",
    "        if 'module.' in list(checkpoint['model_state_dict'].keys())[0]:\n",
    "            new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}\n",
    "            shadow_model.load_state_dict(new_state_dict)\n",
    "        else:\n",
    "            shadow_model.load_state_dict(checkpoint['model_state_dict']) # target_path = os.path.join(self.args.target_path, 'checkpoint.pkl')\n",
    "        # shadow_path = os.path.join(self.args.shadow_path, 'model.pt')\n",
    "        # shadow_model = torch.load(shadow_path).to(self.device)\n",
    "        self.shadow_model = shadow_model\n",
    "        \n",
    "        print('Loading shadow model from ', self.args.shadow_path)\n",
    "        self.model_dict = {'t': self.target_model, 's': self.shadow_model}\n",
    "\n",
    "    def run_blackbox_attacks(self):\n",
    "        \"\"\"Run black-box attacks \"\"\"\n",
    "        # print(self.args.attack_member_loader)\n",
    "        # 使用存储的参数重新创建数据集对象\n",
    "        # subset_dataset = CustomDataset(self.args.subset_data, self.args.subset_labels)\n",
    "        # sampled_dataset = torch.utils.data.Subset(self.target_testloader.dataset, self.args.sampled_indices)\n",
    "\n",
    "        # 使用重新创建的数据集对象创建新的数据加载器\n",
    "        # subset_loader = torch.utils.data.DataLoader(subset_dataset, batch_size=64, shuffle=True)\n",
    "        # sampled_dataloader = torch.utils.data.DataLoader(sampled_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "        t_logits_pos, t_posteriors_pos, t_losses_pos, t_labels_pos = self.get_blackbox_statistics(\n",
    "            self.target_trainloader, self.target_model)\n",
    "        t_logits_neg, t_posteriors_neg, t_losses_neg, t_labels_neg = self.get_blackbox_statistics(\n",
    "            self.target_testloader, self.target_model)\n",
    "        s_logits_pos, s_posteriors_pos, s_losses_pos, s_labels_pos = self.get_blackbox_statistics(\n",
    "            self.shadow_trainloader, self.shadow_model)\n",
    "        s_logits_neg, s_posteriors_neg, s_losses_neg, s_labels_neg = self.get_blackbox_statistics(\n",
    "            self.shadow_testloader, self.shadow_model)\n",
    "\n",
    "        ## metric_based attacks\n",
    "        bb_benchmark = Benchmark_Blackbox(shadow_train_scores=[s_posteriors_pos, s_losses_pos],\n",
    "                                          shadow_test_scores=[s_posteriors_neg, s_losses_neg],\n",
    "                                          target_train_scores=[t_posteriors_pos, t_losses_pos],\n",
    "                                          target_test_scores=[t_posteriors_neg, t_losses_neg])\n",
    "        bb_benchmark.load_labels(s_labels_pos, s_labels_neg, t_labels_pos, t_labels_neg, self.num_classes)\n",
    "        bb_benchmark.compute_bb_scores()\n",
    "\n",
    "        ## nn attack\n",
    "        # info, names, results = self.run_nn_attack(s_logits_pos, s_logits_neg, t_logits_pos, t_logits_neg)\n",
    "\n",
    "        ### Save results\n",
    "        # log_info = info\n",
    "        log_info='null'\n",
    "        # all_names = [names]\n",
    "        all_names=[['null']]\n",
    "        # all_results = [results]\n",
    "        all_results=[['null']]\n",
    "        info, names, results = bb_benchmark.compute_attack_acc()\n",
    "        all_names.append(names)\n",
    "        all_results.append(results)\n",
    "        log_info += info\n",
    "        info, names, results = bb_benchmark.compute_attack_auc()\n",
    "        all_names.append(names)\n",
    "        all_results.append(results)\n",
    "        log_info += info\n",
    "        self.bb_loginfo = log_info\n",
    "        self.bb_results = np.concatenate(all_results)\n",
    "        self.bb_names = np.concatenate(all_names)\n",
    "\n",
    "    def run_whitebox_attacks(self):\n",
    "        \"\"\"Run white-box attacks\"\"\"\n",
    "\n",
    "        def run_case(partition, subset, grad_type):\n",
    "            if partition == 's':\n",
    "                model_dir = self.args.shadow_path\n",
    "            else:\n",
    "                assert partition == 't'\n",
    "                model_dir = self.args.target_path\n",
    "            filename = f'{partition}_{subset}_{grad_type}'\n",
    "            loadername = f'{partition}_{subset}'\n",
    "            path = os.path.join(model_dir, 'attack', filename + '.pkl')\n",
    "\n",
    "            # if os.path.exists(path):\n",
    "            #     stat = unpickle(path)\n",
    "            # else:\n",
    "            if grad_type == 'x':\n",
    "                stat = self.gradient_based_attack_wrt_x(self.loader_dict[loadername], self.model_dict[partition])\n",
    "            else:\n",
    "                assert grad_type == 'w'\n",
    "                stat = self.gradient_based_attack_wrt_w(self.loader_dict[loadername], self.model_dict[partition])\n",
    "            savepickle(stat, path)\n",
    "            return stat\n",
    "\n",
    "        ### Grad w.r.t. x\n",
    "        s_pos_x = run_case('s', 'pos', 'x')\n",
    "        s_neg_x = run_case('s', 'neg', 'x')\n",
    "        t_pos_x = run_case('t', 'pos', 'x')\n",
    "        t_neg_x = run_case('t', 'neg', 'x')\n",
    "\n",
    "        ### Grad w.r.t. w\n",
    "        s_pos_w = run_case('s', 'pos', 'w')\n",
    "        s_neg_w = run_case('s', 'neg', 'w')\n",
    "        t_pos_w = run_case('t', 'pos', 'w')\n",
    "        t_neg_w = run_case('t', 'neg', 'w')\n",
    "\n",
    "        ### Save results\n",
    "        all_names = []\n",
    "        all_results = []\n",
    "        log_info = ''\n",
    "        wb_benchmark = Benchmark(shadow_train_scores=[s_pos_x['l1'], s_pos_x['l2'], s_pos_w['l1'], s_pos_w['l2']],\n",
    "                                 shadow_test_scores=[s_neg_x['l1'], s_neg_x['l2'], s_neg_w['l1'], s_neg_w['l2']],\n",
    "                                 target_train_scores=[t_pos_x['l1'], t_pos_x['l2'], t_pos_w['l1'], t_pos_w['l2']],\n",
    "                                 target_test_scores=[t_neg_x['l1'], t_neg_x['l2'], t_neg_w['l1'], t_neg_w['l2']])\n",
    "        info, names, results = wb_benchmark.compute_attack_acc(\n",
    "            method_names=['grad_wrt_x_l1 ACC', 'grad_wrt_x_l2 ACC', 'grad_wrt_w_l1 ACC', 'grad_wrt_w_l2 ACC'],\n",
    "            score_signs=['-', '-', '-', '-'])\n",
    "        all_names.append(names)\n",
    "        all_results.append(results)\n",
    "        log_info += info\n",
    "\n",
    "        info, names, results = wb_benchmark.compute_attack_auc(\n",
    "            method_names=['grad_wrt_x_l1 AUC', 'grad_wrt_x_l2 AUC', 'grad_wrt_w_l1 AUC', 'grad_wrt_w_l2 AUC'],\n",
    "            score_signs=['-', '-', '-', '-'])\n",
    "        all_names.append(names)\n",
    "        all_results.append(results)\n",
    "        log_info += info\n",
    "        # print(log_info)\n",
    "        self.wb_loginfo = log_info\n",
    "        self.wb_results = np.concatenate(all_results)\n",
    "        self.wb_names = np.concatenate(all_names)\n",
    "\n",
    "    def save_results(self):\n",
    "        \"\"\"Save to attack_log.txt file and .csv file\"\"\"\n",
    "        with open(os.path.join(self.save_dir, 'attack_log.txt'), 'a+') as f:\n",
    "            log_info = '=' * 100 + '\\n' + self.args.target_path + '\\n' + self.wb_loginfo + '\\n' + self.bb_loginfo\n",
    "            f.writelines(log_info)\n",
    "        write_csv(os.path.join(self.save_dir, 'attack_log.csv'),\n",
    "                  self.args.target_path.split('/')[-1],\n",
    "                  np.concatenate([self.bb_results, self.wb_results]),\n",
    "                  np.concatenate([self.bb_names, self.wb_names]))\n",
    "\n",
    "    def gradient_based_attack_wrt_x(self, dataloader, model):\n",
    "        \"\"\"Gradient w.r.t. input\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        ## store results\n",
    "        names = ['l1', 'l2', 'Min', 'Max', 'Mean', 'Skewness', 'Kurtosis']\n",
    "        all_stats = {}\n",
    "        for name in names:\n",
    "            all_stats[name] = []\n",
    "\n",
    "        ## iterate over batches\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(dataloader)):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ## iterate over samples within a batch\n",
    "            for input, target in zip(inputs, targets):\n",
    "                input = torch.unsqueeze(input, 0)\n",
    "                input.requires_grad = True\n",
    "                output = model(input)\n",
    "                loss = self.crossentropy(output, torch.unsqueeze(target, 0))\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## get gradients\n",
    "                gradient = input.grad.view(-1).cpu().numpy()\n",
    "\n",
    "                ## get statistics\n",
    "                stats = compute_norm_metrics(gradient)\n",
    "                for i, stat in enumerate(stats):\n",
    "                    all_stats[names[i]].append(stat)\n",
    "\n",
    "        for name in names:\n",
    "            all_stats[name] = np.array(all_stats[name])\n",
    "        return all_stats\n",
    "\n",
    "    def gradient_based_attack_wrt_w(self, dataloader, model):\n",
    "        \"\"\"Gradient w.r.t. weights\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        ## store results\n",
    "        names = ['l1', 'l2', 'Min', 'Max', 'Mean', 'Skewness', 'Kurtosis']\n",
    "        all_stats = {}\n",
    "        for name in names:\n",
    "            all_stats[name] = []\n",
    "\n",
    "        ## iterate over batches\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(dataloader)):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ## iterate over samples within a batch\n",
    "            for input, target in zip(inputs, targets):\n",
    "                input = torch.unsqueeze(input, 0)\n",
    "                output = model(input)\n",
    "                loss = self.crossentropy(output, torch.unsqueeze(target, 0))\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                ## get gradients\n",
    "                grads_onesample = []\n",
    "                for param in model.parameters():\n",
    "                    grads_onesample.append(param.grad.view(-1))\n",
    "                gradient = torch.cat(grads_onesample)\n",
    "                gradient = gradient.cpu().numpy()\n",
    "\n",
    "                ## get statistics\n",
    "                stats = compute_norm_metrics(gradient)\n",
    "                for i, stat in enumerate(stats):\n",
    "                    all_stats[names[i]].append(stat)\n",
    "\n",
    "        for name in names:\n",
    "            all_stats[name] = np.array(all_stats[name])\n",
    "        return all_stats\n",
    "\n",
    "    def get_blackbox_statistics(self, dataloader, model):\n",
    "        \"\"\"Compute the blackbox statistics (for blackbox attacks)\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        logits = []\n",
    "        labels = []\n",
    "        losses = []\n",
    "        posteriors = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                # print(item)\n",
    "                inputs, targets = inputs.to(torch.float).to(self.device), targets.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = self.crossentropy_noreduce(outputs, targets)\n",
    "                posterior = self.softmax(outputs)\n",
    "                logits.extend(outputs.cpu().numpy())\n",
    "                posteriors.extend(posterior.cpu().numpy())\n",
    "                labels.append(targets.cpu().numpy())\n",
    "                losses.append(loss.cpu().numpy())\n",
    "        logits = np.vstack(logits)\n",
    "        posteriors = np.vstack(posteriors)\n",
    "        labels = np.concatenate(labels)\n",
    "        losses = np.concatenate(losses)\n",
    "        return logits, posteriors, losses, labels\n",
    "\n",
    "    def run_nn_attack(self, s_logits_pos, s_logits_neg, t_logits_pos, t_logits_neg, if_load_checkpoint=True):\n",
    "        checkpoint_dir = os.path.join(self.args.shadow_path, 'attack', 'nn')\n",
    "        mkdir(checkpoint_dir)\n",
    "        trainer = NNAttackTrainer(self.args, checkpoint_dir)\n",
    "        trainer.set_loader(s_logits_pos, s_logits_neg, t_logits_pos, t_logits_neg)\n",
    "        if os.path.exists(os.path.join(checkpoint_dir, 'attack_model.pt')) and if_load_checkpoint:\n",
    "            checkpoint = torch.load(os.path.join(self.args.shadow_path, 'checkpoint.pkl'))\n",
    "            print(checkpoint.keys())\n",
    "            if 'module.' in list(checkpoint['attack_model_state_dict'].keys())[0]:\n",
    "                new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['attack_model_state_dict'].items()}\n",
    "                attack_model.load_state_dict(new_state_dict)\n",
    "            else:\n",
    "                attack_model.load_state_dict(checkpoint['attack_model_state_dict']) \n",
    "            # target_path = os.path.join(self.args.target_path, 'checkpoint.pkl')\n",
    "            # attack_model = torch.load(os.path.join(checkpoint_dir, 'attack_model.pt')).to(self.device)\n",
    "            print('Load NN attack from checkpoint_dir')\n",
    "        else:\n",
    "            max_epoch = 20\n",
    "            lr = 0.001\n",
    "            attack_model = NNAttack(self.num_classes)\n",
    "            optimizer = optim.Adam(attack_model.parameters(), lr=lr)\n",
    "            logger = trainer.logger\n",
    "            print('Train NN attack')\n",
    "            for _ in range(max_epoch):\n",
    "                train_loss, train_acc = trainer.train(attack_model, optimizer)\n",
    "                test_loss, test_acc, _ = trainer.test(attack_model)\n",
    "                logger.append([train_loss, test_loss, train_acc, test_acc])\n",
    "            torch.save(attack_model, os.path.join(checkpoint_dir, 'attack_model.pt'))\n",
    "        _, attack_acc, attack_auc = trainer.test(attack_model)\n",
    "        info = 'MIA via NN : the attack acc is {acc:.3f} \\n'.format(acc=attack_acc / 100)\n",
    "        info += 'MIA via NN : the attack auc is {auc:.3f} \\n'.format(auc=attack_auc)\n",
    "        return info, ['NN ACC', 'NN AUC'], [attack_acc / 100, attack_auc]\n",
    "\n",
    "def compute_norm_metrics(gradient):\n",
    "    \"\"\"Compute the metrics\"\"\"\n",
    "    l1 = np.linalg.norm(gradient, ord=1)\n",
    "    l2 = np.linalg.norm(gradient)\n",
    "    Min = np.linalg.norm(gradient, ord=-np.inf)  ## min(abs(x))\n",
    "    Max = np.linalg.norm(gradient, ord=np.inf)  ## max(abs(x))\n",
    "    Mean = np.average(gradient)\n",
    "    Skewness = skew(gradient)\n",
    "    Kurtosis = kurtosis(gradient)\n",
    "    return [l1, l2, Min, Max, Mean, Skewness, Kurtosis]\n",
    "\n",
    "class NNAttack(nn.Module):\n",
    "    \"\"\"NN attack model\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, hiddens=[100]):\n",
    "        super(NNAttack, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(hiddens)):\n",
    "            if i == 0:\n",
    "                layer = nn.Linear(input_dim, hiddens[i])\n",
    "            else:\n",
    "                layer = nn.Linear(hiddens[i - 1], hiddens[i])\n",
    "            self.layers.append(layer)\n",
    "        self.last_layer = nn.Linear(hiddens[-1], output_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = self.relu(layer(output))\n",
    "        output = self.last_layer(output)\n",
    "        return output\n",
    "\n",
    "class NNAttackTrainer(BaseTrainer):\n",
    "    \"\"\"Trainer for the NN attack\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def construct_dataloader(stat_pos, stat_neg):\n",
    "        \"\"\"Construct dataloader from statistics\"\"\"\n",
    "        attack_data = np.concatenate([stat_neg, stat_pos], axis=0)\n",
    "        attack_data = np.sort(attack_data, axis=1)\n",
    "        attack_targets = np.concatenate([np.zeros(len(stat_neg)), np.ones(len(stat_pos))])\n",
    "        attack_targets = attack_targets.astype(np.int)\n",
    "        attack_indices = np.arange(len(attack_data))\n",
    "        np.random.shuffle(attack_indices)\n",
    "        attack_data = attack_data[attack_indices]\n",
    "        attack_targets = attack_targets[attack_indices]\n",
    "        tensor_x = torch.from_numpy(attack_data)\n",
    "        tensor_y = torch.from_numpy(attack_targets)\n",
    "        tensor_y = tensor_y.unsqueeze(-1).type(torch.FloatTensor)\n",
    "        attack_dataset = data.TensorDataset(tensor_x, tensor_y)\n",
    "        attack_loader = data.DataLoader(attack_dataset, batch_size=256, shuffle=True,generator=torch.Generator(device = 'cuda:0'))\n",
    "        return attack_loader\n",
    "\n",
    "    def set_loader(self, s_logits_pos, s_logits_neg, t_logits_pos, t_logits_neg):\n",
    "        \"\"\"Set the training and testing dataloader\"\"\"\n",
    "        self.trainloader = self.construct_dataloader(s_logits_pos, s_logits_neg)\n",
    "        self.testloader = self.construct_dataloader(t_logits_pos, t_logits_neg)\n",
    "\n",
    "    def set_criterion(self):\n",
    "        \"\"\"Set the training criterion (BCE by default)\"\"\"\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def train(self, model, optimizer):\n",
    "        \"\"\"Train\"\"\"\n",
    "        model.train()\n",
    "        criterion = self.criterion\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        dataload_time = AverageMeter()\n",
    "        time_stamp = time.time()\n",
    "\n",
    "        bar = Bar('Processing', max=len(self.trainloader))\n",
    "        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            ### Record the data loading time\n",
    "            dataload_time.update(time.time() - time_stamp)\n",
    "\n",
    "            ### Output\n",
    "            outputs = model(inputs)\n",
    "            if outputs.shape[-1] == 1:\n",
    "                outputs = outputs.view(-1)\n",
    "                targets = targets.view(-1)\n",
    "                outputs = nn.Sigmoid()(outputs)\n",
    "                prec1 = accuracy_binary(outputs.data, targets.data)\n",
    "            else:\n",
    "                prec1 = accuracy(outputs.data, targets.data)[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            ### Record accuracy and loss\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "            ### Optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            ### Record the total time for processing the batch\n",
    "            batch_time.update(time.time() - time_stamp)\n",
    "            time_stamp = time.time()\n",
    "\n",
    "            ### Progress bar\n",
    "            bar.suffix = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f}'.format(\n",
    "                batch=batch_idx + 1,\n",
    "                size=len(self.trainloader),\n",
    "                data=dataload_time.avg,\n",
    "                bt=batch_time.avg,\n",
    "                total=bar.elapsed_td,\n",
    "                eta=bar.eta_td,\n",
    "                loss=losses.avg,\n",
    "                top1=top1.avg\n",
    "            )\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        return (losses.avg, top1.avg)\n",
    "\n",
    "    def test(self, model):\n",
    "        \"\"\"Test\"\"\"\n",
    "        model.eval()\n",
    "        criterion = self.criterion\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        batch_time = AverageMeter()\n",
    "        dataload_time = AverageMeter()\n",
    "        time_stamp = time.time()\n",
    "        ytest = []\n",
    "        ypred_score = []\n",
    "\n",
    "        bar = Bar('Processing', max=len(self.testloader))\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.testloader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                ### Record the data loading time\n",
    "                dataload_time.update(time.time() - time_stamp)\n",
    "\n",
    "                ### Forward\n",
    "                outputs = model(inputs)\n",
    "                if outputs.shape[-1] == 1:\n",
    "                    outputs = outputs.view(-1)\n",
    "                    targets = targets.view(-1)\n",
    "                    outputs = nn.Sigmoid()(outputs)\n",
    "                    prec1 = accuracy_binary(outputs.data, targets.data)\n",
    "                    ytest.append(targets.cpu().numpy())\n",
    "                    ypred_score.append(outputs.cpu().numpy())\n",
    "                else:\n",
    "                    prec1 = accuracy(outputs.data, targets.data)[0]\n",
    "                    ytest.append(targets.cpu().numpy())\n",
    "                    outputs = nn.Softmax(dim=1)(outputs)\n",
    "                    ypred_score.append(outputs.cpu().numpy()[:, 1])\n",
    "\n",
    "                ### Evaluate\n",
    "                loss = criterion(outputs, targets)\n",
    "                losses.update(loss.item(), inputs.size(0))\n",
    "                top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "                ### Record the total time for processing the batch\n",
    "                batch_time.update(time.time() - time_stamp)\n",
    "                time_stamp = time.time()\n",
    "\n",
    "                ### Progress bar\n",
    "                bar.suffix = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(self.testloader),\n",
    "                    data=dataload_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                )\n",
    "                bar.next()\n",
    "            bar.finish()\n",
    "        ytest = np.concatenate(ytest)\n",
    "        ypred_score = np.concatenate(ypred_score)\n",
    "        auc, ap, f1, pos_num, frac = metrics_binary(ytest, ypred_score)\n",
    "        return (losses.avg, top1.avg, auc)\n",
    "\n",
    "    def set_logger(self):\n",
    "        \"\"\"Set up logger\"\"\"\n",
    "        title = self.args.dataset\n",
    "        self.start_epoch = 0\n",
    "        logger = Logger(os.path.join(self.save_dir, 'log.txt'), title=title)\n",
    "        logger.set_names(['Train Loss', 'Val Loss', 'Train Acc', 'Val Acc'])\n",
    "        self.logger = logger\n",
    "\n",
    "class Attacker(BaseAttacker):\n",
    "    def __init__(self, args, save_dir, member, nonmember):\n",
    "        super().__init__(args, save_dir, member, nonmember)\n",
    "\n",
    "    def set_dataloader(self):\n",
    "        \"\"\"The function to set the dataset parameters\"\"\"\n",
    "        self.data_root = '/data/home/huqiang/DeepCore/RelaxLoss/data'\n",
    "        if self.args.dataset == 'CIFAR10':\n",
    "            self.dataset = CIFAR10\n",
    "            self.num_classes = 10\n",
    "            self.dataset_size = 60000\n",
    "        elif self.args.dataset == 'CIFAR100':\n",
    "            self.dataset = CIFAR100\n",
    "            self.num_classes = 100\n",
    "            self.dataset_size = 60000\n",
    "        transform_train = transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                                    (0.2023, 0.1994, 0.2010))])\n",
    "        self.transform_train = transform_train\n",
    "        self.transform_test = transform_test\n",
    "\n",
    "        ## Set the partition and datloader\n",
    "        # indices = np.load(os.path.join(self.args.target_path, 'full_idx.npy'))\n",
    "        indices = np.load('/data/home/huqiang/DeepCore/RelaxLoss/results/CIFAR10/resnet20/advreg/full_idx.npy')\n",
    "        if os.path.exists(os.path.join(self.args.shadow_path, 'full_idx.npy')):\n",
    "            shadow_indices = np.load(os.path.join(self.args.shadow_path, 'full_idx.npy'))\n",
    "            # print(\"indices\")\n",
    "            # print(indices)\n",
    "            # print(shadow_indices)\n",
    "            assert np.array_equiv(indices, shadow_indices)\n",
    "        self.partition = Partition(dataset_size=self.dataset_size, indices=indices)\n",
    "        target_train_idx, target_test_idx = self.partition.get_target_indices()\n",
    "        shadow_train_idx, shadow_test_idx = self.partition.get_shadow_indices()\n",
    "\n",
    "        sorted_indices=np.load(\"/data/home/huqiang/DeepCore/save/indices_uncertainty/sorted_indices.npy\")\n",
    "        target_trainset = self.dataset(root=self.data_root, indices=self.member,\n",
    "                                       download=True, transform=self.transform_train)\n",
    "        target_testset = self.dataset(root=self.data_root, indices=self.nonmember,\n",
    "                                      download=True, transform=self.transform_test)\n",
    "        shadow_trainset = self.dataset(root=self.data_root, indices=shadow_train_idx,\n",
    "                                       download=True, transform=self.transform_train)\n",
    "        shadow_testset = self.dataset(root=self.data_root, indices=shadow_test_idx,\n",
    "                                      download=True, transform=self.transform_test)\n",
    "        self.target_trainloader = torch.utils.data.DataLoader(target_trainset, batch_size=self.args.test_batchsize, shuffle=False)\n",
    "        self.target_testloader = torch.utils.data.DataLoader(target_testset, batch_size=self.args.test_batchsize, shuffle=False)\n",
    "        self.shadow_trainloader = torch.utils.data.DataLoader(shadow_trainset, batch_size=self.args.test_batchsize, shuffle=False)\n",
    "        self.shadow_testloader = torch.utils.data.DataLoader(shadow_testset, batch_size=self.args.test_batchsize, shuffle=False)\n",
    "        self.loader_dict = {'s_pos': self.shadow_trainloader, 's_neg': self.shadow_testloader,\n",
    "                            't_pos': self.target_trainloader, 't_neg': self.target_testloader}\n",
    "\n",
    "def check_args(parser):\n",
    "    '''check and store the arguments as well as set up the save_dir'''\n",
    "    ## set up save_dir\n",
    "    args = parser.parse_args()\n",
    "    save_dir = os.path.join(args.target_path, 'attack')\n",
    "    mkdir(save_dir)\n",
    "    mkdir(os.path.join(args.shadow_path, 'attack'))\n",
    "\n",
    "    ## load configs and store the parameters\n",
    "    preload_configs = load_yaml(os.path.join(args.target_path, 'params.yml'))\n",
    "    parser.set_defaults(**preload_configs)\n",
    "    args = parser.parse_args()\n",
    "    write_yaml(vars(args), os.path.join(save_dir, 'params.yml'))\n",
    "    return args, save_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide the training set into n subsets based on importance scores\n",
    "def divide_into_subsets(importance_scores, n, type='size'):\n",
    "    if type=='len':\n",
    "        min_score = np.min(importance_scores)\n",
    "        max_score = np.max(importance_scores)\n",
    "        intervals = np.linspace(min_score, max_score, n + 1)\n",
    "        subset_indices = np.digitize(importance_scores, intervals) - 1\n",
    "    else:\n",
    "        # Sort the importance scores and find the indices that would sort the array\n",
    "        sorted_indices = np.argsort(importance_scores)\n",
    "\n",
    "        # Calculate the number of data points in each subset\n",
    "        subset_size = len(importance_scores) // n\n",
    "\n",
    "        # Create an array to hold the subset indices for each data point\n",
    "        subset_indices = np.zeros(len(importance_scores), dtype=np.int64)\n",
    "\n",
    "        np.save(f\"/data/home/huqiang/DeepCore/save/indices_uncertainty/sorted_indices.npy\",sorted_indices)\n",
    "\n",
    "        # Assign data points to subsets based on sorted indices\n",
    "        for i in range(n):\n",
    "            start_idx = i * subset_size\n",
    "            end_idx = (i + 1) * subset_size\n",
    "            np.save(f\"/data/home/huqiang/DeepCore/save/indices_uncertainty/sorted_indices{i}_10.npy\",sorted_indices[start_idx:end_idx])\n",
    "            subset_indices[sorted_indices[start_idx:end_idx]] = i\n",
    "\n",
    "        # For any remaining data points, assign them to the last subset\n",
    "        print(subset_indices)\n",
    "        subset_indices[sorted_indices[end_idx:]] = n - 1\n",
    "\n",
    "    return subset_indices\n",
    "\n",
    "def get_subset_midpoints(importance_scores, subset_indices, n_subsets,type='len'):\n",
    "    if type=='size':\n",
    "        subset_midpoints = []\n",
    "        for i in range(n_subsets):\n",
    "            subset_mask = subset_indices == i\n",
    "            subset_scores = importance_scores[subset_mask]\n",
    "            if len(subset_scores) > 0:\n",
    "                midpoint = np.mean(subset_scores)\n",
    "            else:\n",
    "                midpoint = 0.0  # Handle empty subsets if necessary\n",
    "            subset_midpoints.append(midpoint)\n",
    "    else:\n",
    "        # Calculate the range of importance scores\n",
    "        min_score = np.min(importance_scores)\n",
    "        max_score = np.max(importance_scores)\n",
    "\n",
    "        # Calculate the step size for equally spaced intervals\n",
    "        step_size = (max_score - min_score) / n_subsets\n",
    "\n",
    "        # Calculate the midpoints of the importance score intervals\n",
    "        subset_midpoints = np.linspace(min_score + step_size / 2, max_score - step_size / 2, n_subsets)\n",
    "\n",
    "    return np.array(subset_midpoints)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_item = self.data[index]\n",
    "        label_item = self.labels[index]\n",
    "        return data_item, label_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mia_by_loss(model,subset_loader,test_loader):\n",
    "    subset_train_losses = compute_losses(model, subset_loader)\n",
    "    n_samples_in_subset = len(subset_train_losses)\n",
    "        \n",
    "    # Randomly select n_samples_in_subset samples from the test set\n",
    "    test_losses=compute_losses(model,test_loader)\n",
    "    np.random.shuffle(test_losses)\n",
    "    test_losses=test_losses[:n_samples_in_subset]\n",
    "\n",
    "    # Merge the subset with the test samples\n",
    "    merged_losses = np.concatenate((test_losses, subset_train_losses)).reshape((-1, 1))\n",
    "\n",
    "    # Create membership labels for the merged data\n",
    "    merged_labels = np.array([0] * len(test_losses) + [1] * len(subset_train_losses))\n",
    "    merged_labels = torch.tensor(merged_labels)\n",
    "\n",
    "    # Perform the membership inference attack for the merged data\n",
    "    mia_scores_subset = simple_mia(merged_losses, merged_labels)\n",
    "    return mia_scores_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_defense(dataset_prefix, save_root, args, method):\n",
    "    model_flag = (dataset_prefix == 'cifar')\n",
    "    if method == 'distillation':\n",
    "        ## train teacher model\n",
    "        teacher_path = os.path.join(save_root, 'vanilla', f'seed{args.seed}')\n",
    "        if not os.path.exists(os.path.join(teacher_path, 'model.pt')):\n",
    "            command = f'python {dataset_prefix}/defense/vanilla.py -name seed{args.seed} -s {args.seed} --dataset {args.dataset}'\n",
    "            command += f' --model {args.model}' if model_flag else ''\n",
    "            os.system(command)\n",
    "\n",
    "        ## train student model\n",
    "        command = f'python {dataset_prefix}/defense/{method}.py -name seed{args.seed} -s {args.seed} --dataset {args.dataset} -teacher {teacher_path}'\n",
    "        command += f' --model {args.model}' if model_flag else ''\n",
    "        os.system(command)\n",
    "\n",
    "    else:\n",
    "        command = f'python {dataset_prefix}/defense/{method}.py -name seed{args.seed} -s {args.seed} --dataset {args.dataset}'\n",
    "        command += f' --model {args.model}' if model_flag else ''\n",
    "        os.system(command)\n",
    "\n",
    "def run_shadow(dataset_prefix, save_root, args, method):\n",
    "    model_flag = (dataset_prefix == 'cifar')\n",
    "    if method == 'distillation':\n",
    "        ## train teacher model\n",
    "        teacher_path = os.path.join(save_root, 'vanilla', f'seed{args.seed}', 'shadow')\n",
    "        if not os.path.exists(os.path.join(teacher_path, 'model.pt')):\n",
    "            command = f'python RelaxLoss/source/{dataset_prefix}/defense/vanilla.py -name seed{args.seed} -s {args.seed} --dataset {args.dataset} --partition shadow'\n",
    "            command += f' --model {args.model}' if model_flag else ''\n",
    "            os.system(command)\n",
    "\n",
    "        ## train student model\n",
    "        command = f'python RelaxLoss/source/{dataset_prefix}/defense/{method}.py -name seed{args.seed} -s {args.seed} ' \\\n",
    "                  f'--dataset {args.dataset} -teacher {teacher_path} --partition shadow'\n",
    "        command += f' --model {args.model}' if model_flag else ''\n",
    "        os.system(command)\n",
    "\n",
    "    else:\n",
    "        command = f'python RelaxLoss/source/{dataset_prefix}/defense/{method}.py -name seed{args.seed} -s {args.seed} --dataset {args.dataset} --partition shadow'\n",
    "        command += f' --model {args.model}' if model_flag else ''\n",
    "        os.system(command)\n",
    "\n",
    "def run_attack(dataset_prefix, target, shadow, member,nonmember):\n",
    "    save_dir = os.path.join(target, 'attack')\n",
    "    mkdir(save_dir)\n",
    "    mkdir(os.path.join(shadow, 'attack'))\n",
    "\n",
    "    ## load configs and store the parameters\n",
    "    preload_configs = load_yaml(os.path.join(target, 'params.yml'))\n",
    "    # parser.set_defaults(**preload_configs)\n",
    "    # args = parser.parse_args()\n",
    "    write_yaml(vars(args), os.path.join(save_dir, 'params.yml'))\n",
    "\n",
    "    args.target_path=target\n",
    "    args.shadow_path=shadow\n",
    "\n",
    "    attacker = Attacker(args, save_dir ,member,nonmember)\n",
    "    # attacker.target_trainloader\n",
    "    attacker.run_blackbox_attacks()\n",
    "    attacker.run_whitebox_attacks()\n",
    "    attacker.save_results()\n",
    "    # command = (\n",
    "    # f'python RelaxLoss/source/{dataset_prefix}/run_attacks.py '\n",
    "    # f'-target \"{target}\" -shadow \"{shadow}\" --subset_data \"{subset_data}\" --subset_labels \"{subset_labels}\" --sampled_indices \"{sampled_indices}\"'\n",
    "    # )\n",
    "\n",
    "    # os.system(command)\n",
    "    # os.system(f'python RelaxLoss/source/{dataset_prefix}/run_attacks.py -target {target} -shadow {shadow} -member {member} -nonmember {nonmember}')\n",
    "\n",
    "def attack(args,member=None,nonmember=None):\n",
    "    FILE_DIR = os.path.dirname(os.path.abspath(\"/data/home/huqiang/DeepCore/mia_scoring.ipynb\"))\n",
    "    SAVE_ROOT_IMAGE = os.path.join(FILE_DIR, 'RelaxLoss/results/%s/%s/')\n",
    "    SAVE_ROOT_GENERAL = os.path.join(FILE_DIR, 'RelaxLoss/results/%s/')\n",
    "\n",
    "    # args = parse_arguments()\n",
    "    if args.dataset in ['CIFAR10', 'CIFAR100']:\n",
    "        dataset_prefix = 'cifar'\n",
    "        save_root = SAVE_ROOT_IMAGE % (args.dataset, args.model)\n",
    "    elif args.dataset in ['Texas', 'Purchase']:\n",
    "        dataset_prefix = 'nonimage'\n",
    "        save_root = SAVE_ROOT_GENERAL % args.dataset\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    ## train shadow model (for attack)\n",
    "    base_shadow_path = os.path.join(save_root, 'vanilla', f'seed{args.seed}', 'shadow')\n",
    "    if not os.path.exists(os.path.join(base_shadow_path, 'model.pt')):\n",
    "        run_shadow(dataset_prefix, save_root, args, 'vanilla')\n",
    "\n",
    "    ## run attack\n",
    "    # target_path = os.path.join(save_root, args.method, f'seed{args.seed}')\n",
    "    target_path=\"/data/home/huqiang/DeepCore/RelaxLoss/results/CIFAR10/resnet20/label_smoothing/all_defense_300\"\n",
    "    if args.method == 'early_stopping':\n",
    "        all_targets = [p for p in os.listdir(target_path) if os.path.isdir(os.path.join(target_path,p)) and 'ep' in p]\n",
    "        for target_path in all_targets:\n",
    "            run_attack(dataset_prefix, target_path, base_shadow_path, member,nonmember)\n",
    "    else:\n",
    "        run_attack(dataset_prefix, target_path, base_shadow_path, member,nonmember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_membership_inference_attack(model, train_loader, test_loader, n, method='uncertainty', type='size'):\n",
    "    # Step 1: Importance scoring on the training set\n",
    "    # if method=='uncertainty':\n",
    "    #     uncertainty = metrics.Uncertainty(model, selection_method=\"Margin\")\n",
    "    #     importance_scores = uncertainty.rank_uncertainty(train_loader)\n",
    "    # elif method=='forgetting':\n",
    "    #     forgetting_model = metrics.Forgetting(dst_train=train_loader.dataset, args=args, balance=True)\n",
    "    #     importance_scores=forgetting_model.calculate_importance_scores(model, train_loader)\n",
    "    # elif method=='grand':\n",
    "    #     grand_model = metrics.GraNd(dst_train=train_loader.dataset, args=args, balance=True)\n",
    "    #     importance_scores = grand_model.calculate_importance_scores_gradients(model, train_loader)\n",
    "    # else:\n",
    "    args.model=\"ResNet18\"\n",
    "    importance_scores = metric.calculate_importance_scores(model,method)\n",
    "\n",
    "    # dst_subset = torch.utils.data.Subset(train_loader.dataset, subset[\"indices\"])\n",
    "    print(len(importance_scores))\n",
    "    print(\"+-+-+-\")\n",
    "    print(importance_scores)\n",
    "\n",
    "    # Step 2: Divide the training set into n subsets based on importance scores\n",
    "    subset_indices = divide_into_subsets(importance_scores, n, type)\n",
    "\n",
    "    # Step 3 and Step 4: Perform membership inference attack and compute attack scores for each subset\n",
    "    mia_scores = []\n",
    "    subset_loaders=[]\n",
    "    args.model=\"resnet20\"\n",
    "    for subset_idx in range(n):\n",
    "        print(train_loader.dataset.data.shape)\n",
    "        print(subset_indices.shape)\n",
    "        subset_data = torch.tensor(train_loader.dataset.data)[(subset_indices == subset_idx)]\n",
    "        # print(len(train_loader.dataset.targets))\n",
    "        subset_labels = torch.tensor(train_loader.dataset.targets)[(subset_indices == subset_idx)]\n",
    "        print(subset_data.shape,subset_labels.shape)\n",
    "        subset_dataset = CustomDataset(subset_data, subset_labels)\n",
    "        subset_loader = DataLoader(subset_dataset, batch_size=64, shuffle=False)\n",
    "        subset_loader.dataset.data = subset_loader.dataset.data.permute(0, 3, 1, 2)\n",
    "        \n",
    "        total_samples = len(test_loader.dataset)\n",
    "        num_samples_to_sample =subset_loader.dataset.data.shape[0]\n",
    "        sampled_indices = random.choices(range(total_samples), k=num_samples_to_sample)\n",
    "        sampled_dataset = Subset(test_loader.dataset, sampled_indices)\n",
    "        sampled_dataloader = DataLoader(sampled_dataset, batch_size=64, shuffle=True, num_workers=4,generator=torch.Generator(device='cuda:0'))\n",
    "\n",
    "        # mia_scores_subset=mia_by_loss(model,subset_loader,test_loader)\n",
    "        subset_loaders.append(subset_loader)\n",
    "        # attack(args,subset_loader,sampled_dataloader)\n",
    "\n",
    "        # mia_scores.append(mia_scores_subset.mean())\n",
    "    ###\n",
    "    # with open('/data/home/huqiang/DeepCore/save/subset_loaders.pkl', 'wb') as f:\n",
    "    #     pickle.dump(subset_loaders, f)\n",
    "    ###\n",
    "    \n",
    "    args.model=\"ResNet18\"\n",
    "    # Step 5: Output the midpoints of the importance score intervals and the corresponding attack scores\n",
    "    # importance_intervals = np.linspace(0, 1, n + 1)[:-1] + 0.5 / n\n",
    "    subset_midpoints = get_subset_midpoints(importance_scores, subset_indices, n,type)\n",
    "\n",
    "    return subset_midpoints, np.array(mia_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "10\n",
      "Files already downloaded and verified\n",
      "+++++++++++\n",
      "| Selecting for batch [  1/ 47]\n",
      "| Selecting for batch [ 21/ 47]\n",
      "| Selecting for batch [ 41/ 47]\n",
      "12000\n",
      "12000\n",
      "+-+-+-\n",
      "[0.06533795 0.05212412 0.06317884 ... 0.07347428 0.06407742 0.055045  ]\n",
      "[5 0 4 ... 8 5 0]\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "(12000, 32, 32, 3)\n",
      "(12000,)\n",
      "torch.Size([1200, 32, 32, 3]) torch.Size([1200])\n",
      "Importance Score Intervals (Midpoints): [0.05279013 0.05659922 0.05879035 0.06071242 0.06258057 0.06443653\n",
      " 0.06638084 0.06880136 0.07222965 0.07969248]\n",
      "Membership Inference Attack Scores: []\n"
     ]
    }
   ],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test = datasets.__dict__[args.dataset] \\\n",
    "            (args.data_path)\n",
    "args.channel, args.im_size, args.num_classes, args.class_names = channel, im_size, num_classes, class_names\n",
    "print(args.num_classes)\n",
    "\n",
    "selection_args = dict(epochs=args.selection_epochs,\n",
    "                                  selection_method=args.uncertainty,\n",
    "                                  balance=args.balance,\n",
    "                                  greedy=args.submodular_greedy,\n",
    "                                  function=args.submodular\n",
    "                                  )\n",
    "###\n",
    "transform_train = transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "                                                               transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                                    (0.2023, 0.1994, 0.2010))])\n",
    "indices=np.load('/data/home/huqiang/DeepCore/RelaxLoss/results/CIFAR10/resnet20/advreg/full_idx.npy')\n",
    "partition = Partition(dataset_size=60000, indices=indices)\n",
    "trainset_idx, testset_idx = partition.get_target_indices()\n",
    "target_trainset = CIFAR10(root='/data/home/huqiang/DeepCore/RelaxLoss/data', indices=trainset_idx,\n",
    "                                       download=True, transform=transform_train)    \n",
    "target_trainloader = torch.utils.data.DataLoader(target_trainset, batch_size=64, shuffle=False)                           \n",
    "###\n",
    "\n",
    "# method = methods.__dict__[args.selection](target_trainloader, args, args.fraction, args.seed, **selection_args)\n",
    "# subset = method.select()\n",
    "\n",
    "n_subsets = 10  # You can change the number of subsets as desired\n",
    "importance_intervals, mia_scores = perform_membership_inference_attack(model, target_trainloader, test_loader, n_subsets, method=methods.Uncertainty(dst_train=target_trainset,train_loader=target_trainloader, args=args, fraction=1,random_seed=42,epochs=20,balance=False) )\n",
    "# importance_intervals, mia_scores = perform_membership_inference_attack(model, train_loader, test_loader, n_subsets, method=\"uncertainty\")\n",
    "torch.save(model.state_dict(), './save/model.pt')\n",
    "# Print the results\n",
    "print(\"Importance Score Intervals (Midpoints):\", importance_intervals)\n",
    "print(\"Membership Inference Attack Scores:\", mia_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Capture Output v0.0.11\n"
     ]
    }
   ],
   "source": [
    "import jupyter_capture_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved by overwring previous file at /data/home/huqiang/DeepCore/save/300_label_smoothing_10.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [01:14<00:00,  2.51it/s]\n",
      "100%|██████████| 188/188 [01:19<00:00,  2.35it/s]\n",
      "100%|██████████| 19/19 [00:08<00:00,  2.28it/s]\n",
      "100%|██████████| 19/19 [00:07<00:00,  2.64it/s]\n",
      "100%|██████████| 188/188 [01:38<00:00,  1.92it/s]\n",
      "100%|██████████| 188/188 [01:36<00:00,  1.95it/s]\n",
      "100%|██████████| 19/19 [00:09<00:00,  2.04it/s]\n",
      "100%|██████████| 19/19 [00:09<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "%%capture_text --path \"/data/home/huqiang/DeepCore/save/300_label_smoothing_10.txt\"\n",
    "args.model=\"resnet20\"\n",
    "for i in range(10):\n",
    "    member_indices=np.load(f\"/data/home/huqiang/DeepCore/save/indices_uncertainty/sorted_indices{i}_10.npy\")\n",
    "    nonmember_indices= np.random.choice(range(12000, 24000), size=1200, replace=False)\n",
    "    attack(args,member_indices,nonmember_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "# del tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7609 5456 7894 ... 1538  813 5292]\n",
      "[    0  7994  7995 ...  4005  3996 11999]\n"
     ]
    }
   ],
   "source": [
    "sorted_indices_1=np.load(\"/data/home/huqiang/DeepCore/save/cal/sorted_indices.npy\")\n",
    "sorted_indices_2=np.load(\"/data/home/huqiang/DeepCore/save/forgetting/sorted_indices.npy\")\n",
    "print(sorted_indices_1)\n",
    "print(sorted_indices_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('huqiang')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe7929ab8a9d15375a2db1d39105be1cb17bab0c7279712f98076f9a5bd02dd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
